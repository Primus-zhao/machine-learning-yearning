# 27. 减少方差的技巧
如果你的学习算法遭受高方差，你可以尝试以下技巧：

- **添加更多训练数据**：只要你能够获取更多的数据和处理这些数据的充足计算能力，这是处理方差问题最简单也是最可靠的方式。
- **添加正则化**（L2正则化，L1正则化，dropout）：这将减少了方差，但增加了偏差。
- **添加提前停止（early stopping）**（基于开发集误差提前停止梯度下降）：该技巧减少方差但增加了偏差。提前停止的行为很像正则化方法，所以一些作者也称它为一种正则化技术。


- **特征选择以减少输入特征的数目/类型**：该方法可能有助于解决方差问题，但也可能增加偏差。略微减少特征数量（比如从1000个特征减少到900）不太可能对偏差产生很大影响。显著地减少它（比如从1000减少到100 --- 减少10倍）更可能有显著的影响，因为你可能排除了太多有用的特征。在现代深度学习中，当数据富足时，特征选择的操作便可移除了，现在我们更有可能给算法喂入所有的特征，并让算法根据数据来选择使用哪些特征。但是当你训练集比较小时，特征选择将是非常有用。

**减少模型大小**（如神经元/层的数量）：谨慎使用。该方法能够减少方差，但可能增加偏差。但是，我不推荐使用该方法来处理方差。加入正则化通常能得到更好的分类性能。减少模型大小的好处就是降低计算成本，从而加快模型的训练速度。如果加快模型的训练是有用的，那无论如何都考虑减少模型的大小。但是如果你的目标是减少方差，并且你不关心计算成本，那么请考虑添加正则化替代之。

这里有两个额外的策略，重复上一章处理偏差中的方法：

- **基于误差分析的洞见修改输入特征**：假设误差分析启发你去创建额外的特征，以帮助算法消除特定类别的错误。（我们将在下一章节进一步讨论）这些新特征能有助于减少偏差和方差。理论上来说，增加更多的特征可能会增加方差，但如果你发现这种情况，那么就使用正则化方法，它通常能够消除方差的增加。
- **修改模型架构**（如神经网络架构）以便更适合你的问题：这种方法能够影响偏差和方差。