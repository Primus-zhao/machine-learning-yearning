# 29. 绘制训练误差曲线
你的开发集（和测试集）误差应该随着训练集大小的增长而减少。但随着训练集大小的增加，训练集误差通常会增加。

让我们举例说明这个效果。假设你的训练集有只有两个样本：一张猫咪图和一张不是猫咪图。学习算法很容易“记住”训练集中这两个样本，并且训练集错误率为0%。即使有一张或两张样本图片都被错误标注，算法仍然很容易记住这两个标签。

现在假设你的训练集有100个样本。可能有一些样本是被错误标记或模棱两可的 --- 一些图非常模糊，甚至人都不能区分是否有猫咪。或许学习算法仍能“记住”大部分或所有的训练集，但现在很难获得100%的准确率。通过将训练集样本数从2增加到100，你将发现训练集准确率将略有下降。

最后，假设你的训练集有10000个样本。这种情况下算法更难以完全拟合所有10000个样本，特别是有一些样本是模棱两可或错误标注的。因此，你的学习算法在该训练集上将做的更糟。

让我们为之前的曲线（开发误差曲线）添加训练误差曲线：

<p align="center">
    <img src="figs/learning_curve4.jpg" height="70%" width="70%">
</p>

你可以看到蓝色的“训练误差”曲线随着训练集大小的增加而增加。而且，算法通常在训练集上表现比在开发集上要好。因此，红色的开发误差曲线通常严格地在蓝色训练误差曲线上方。

下一步我们将讨论如何解释这些曲线。