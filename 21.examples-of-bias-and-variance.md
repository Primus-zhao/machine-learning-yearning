# 21. 偏差和方差的例子
考虑我们的猫咪分类任务。一个“理想的”分类器（比如人类）在这个任务中可能取得近乎完美的表现。

假设你的算法表现如下：

- 训练误差 = 1%
- 开发误差 = 11%

它有什么问题？应用前一章的定义，我们估计偏差为1%，方差为10% (=11%-1%)。因此，它有一个很高的方差 **(high variance)**。分类器训练误差非常低，但是没能成功泛化到开发集上。这也被叫做过拟合 **(overfitting)**。

现在，考虑如下：

- 训练误差 = 15%
- 开发误差 = 16%

我们估计偏差为15%，方差为1%。该分类器在训练集拟合得很差，错误率已经达到15%，而它在开发集上的错误几乎没有比在训练集误差更高。因此，该分类器具有较高的偏差 **(high bias)**，但是较低的方差。我们称该算法欠拟合 **(underfitting)**。

现在，考虑如下：

- 训练误差 = 15%
- 开发误差 = 30%

我们估计偏差为15%，方差为15%。该分类器有高偏差和高方差 **(high bias and high variance)**：它在训练集上表现很差，因此有很高的偏差，并且它在开发集上表现更差，因此具有很高的方差。过拟合/欠拟合术语很难应用于此，因为分类器此时同时过拟合和欠拟合了。

最后，考虑如下：

- 训练误差 = 0.5%
- 开发误差 = 1%

该分类器表现很好，它具有低偏差和低方差。若取得这么好的性能，那就恭喜了！