# 20. 偏差和方差：误差的两大来源
假设你的训练集、开发集和测试集都来自同一分布。那么你应该总是试图去获取更多的训练数据，因为这样只会提高性能，对吗？

尽管有更多的数据是无害的，不幸的是，它并不总是如我们期望的那样有帮助。有时候获取更多的数据可能只是在浪费时间。那么，你如何决定何时该添加数据而何时又不必这么做呢？

机器学习中有两个主要误差来源：偏差和方差。理解它们将有助于你决定是否要添加数据，以及利用号时间去执行其他策略来提升性能。

假设你希望构建一个5%误差的猫咪识别器。而目前训练集错误率为15%，开发集错误率为16%。在这种情况下，添加数据可能不会有太多帮助。你应该关注其他改变。实际上，在你的训练集上添加更多的样本只会让你的算法难以在训练集上做的更好。（我们在后面章节中解释了原因）

如果你在训练集上的错误率是15%（85%的准确率），但是你的目标是5%错误率（95%准确率），那么第一个要解决的问题是提高算法在训练集上的性能。算法在开发/测试集上的性能通常比在训练集上要差。所以，如果算法在见过的样本上仅得到85%的准确率，那么是不可能在没见过的样本上得到95%的准确率的。

如上所述，假设你的算法在开发集上有16%的错误率（84%的准确率）。我们将这16%的错误分为两部分：

- 第一部分是算法在训练集上的错误率。在本例中，它是15%。我们非正式地将此认为是算法的 **偏差 (bias)**。
- 第二部分是算法在开发（或测试）集上表现比训练集上差多少。在本例中，开发集表现比训练集差1%。我们非正式地将此认为是算法的 **方差(variance)**。

> 在统计领域有更多关于偏差和方差的正式定义，但不必为此担心。粗略地说，当你有一个非常大的训练集时，偏差就是算法在训练集上的错误率。方差是与此设置中的训练集相比，算法在测试集上差多少。当你使用均方误差 (mean squared error, MSE) 来作为误差度量时，你可以写下偏差和方差对应的两个公式，并证明 **总误差=偏差+方差 (Total Error = Bias + Variance)**。但是为了决定如何在ML问题上取得进展的目的，这里只要给出偏差和方差的非正式的定义就足够了。

学习算法的一些改变能解决误差来源的第一部分 --- 偏差，并且提高算法在训练集上的性能；而一些改变能解决误差来源的第二部分 --- 方差，并帮助算法从训练集到开发/测试集上更好地泛化。为了选择最有希望的改变，了解这两类错误来源中哪个更亟待解决是非常有用的。

> 还有一些方法能够通过对系统架构做出较大的改变，实现同时减少偏差和方差。但是这些方法往往难以识别和实现。

建立对偏差和方差的良好直觉将帮助你为算法选择有效的改变。
